{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/abaghel/projects/rnn-patient-sentiment-paper\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "print os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import cPickle\n",
    "from collections import defaultdict\n",
    "import re\n",
    "from keras.preprocessing import sequence\n",
    "\n",
    "\n",
    "def RepresentsInt(s):\n",
    "    try: \n",
    "        int(s)\n",
    "        return True\n",
    "    except ValueError:\n",
    "        return False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total lines: 44804 \n",
      "Valid lines: 38200\n",
      "Text:  38200\n",
      "Helpful 38200\n",
      "Helpful_cat 38200\n",
      "Staff 38200\n",
      "Staff_cat 38200\n",
      "Knowledge 38200\n",
      "Knowledge_cat 38200\n"
     ]
    }
   ],
   "source": [
    "text=[]\n",
    "helpful=[]\n",
    "helpful_categorical = []\n",
    "knowledge=[]\n",
    "staff=[]\n",
    "knowledge_categorical=[]\n",
    "staff_categorical=[]\n",
    "ii=0\n",
    "jj=0\n",
    "files = [os.path.join(\"data/\", f) for f in os.listdir('data/') if os.path.isfile(os.path.join(\"data/\", f))]\n",
    "for f in files:\n",
    "    if f.endswith(\".txt\"):\n",
    "        with open(f) as tsv:\n",
    "            for line in csv.reader(tsv, dialect=\"excel-tab\"):\n",
    "                ii=ii+1\n",
    "                if(len(line) > 6 and RepresentsInt(line[3]) and RepresentsInt(line[5]) and RepresentsInt(line[6]) ):\n",
    "                    jj=jj+1\n",
    "#                     print (line[3], line[5], int(line[6]))\n",
    "                    if int(line[3])>=3:\n",
    "                        staff.append(int(1))\n",
    "                    if int(line[3])<3:\n",
    "                        staff.append(int(0))\n",
    "                    if int(line[5])>=3:\n",
    "                        helpful.append(int(1))\n",
    "                    if int(line[5])<3:\n",
    "                        helpful.append(int(0))                        \n",
    "                    if int(line[6])>=3:\n",
    "                        knowledge.append(int(1))\n",
    "                    if int(line[6])<3:\n",
    "                        knowledge.append(int(0))                                            \n",
    "                    helpful_categorical.append(int(line[5])-1)\n",
    "                    knowledge_categorical.append(int(line[6])-1)\n",
    "                    staff_categorical.append(int(line[3])-1)\n",
    "                    text.append(line[2])\n",
    "        \n",
    "print \"Total lines:\", ii ,\"\\nValid lines:\", jj        \n",
    "print \"Text: \",len(text)\n",
    "\n",
    "print \"Helpful\", len(helpful)\n",
    "print \"Helpful_cat\", len(helpful_categorical)\n",
    "\n",
    "print \"Staff\", len(staff)\n",
    "print \"Staff_cat\", len(staff_categorical)\n",
    "\n",
    "print \"Knowledge\", len(knowledge)\n",
    "print \"Knowledge_cat\", len(knowledge_categorical)\n",
    "\n",
    "\n",
    "text_train=text[:34000]\n",
    "text_test=text[34000:]\n",
    "\n",
    "helpful_train=helpful[:34000]\n",
    "helpful_test=helpful[34000:]\n",
    "\n",
    "helpful_cat_train=helpful_categorical[:34000]\n",
    "helpful_cat_test=helpful_categorical[34000:]\n",
    "\n",
    "staff_train=staff[:34000]\n",
    "staff_test=staff[34000:]\n",
    "\n",
    "staff_cat_train=staff_categorical[:34000]\n",
    "staff_cat_test=staff_categorical[34000:]\n",
    "\n",
    "\n",
    "knowledge_train=knowledge[:34000]\n",
    "knowledge_test=knowledge[34000:]\n",
    "\n",
    "knowledge_cat_train=knowledge_categorical[:34000]\n",
    "knowledge_cat_test=knowledge_categorical[34000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def clean_str(string):\n",
    "    \"\"\"\n",
    "    Tokenization/string cleaning for dataset\n",
    "    Every dataset is lower cased except\n",
    "    \"\"\"\n",
    "    string = re.sub(r\"[^A-Za-z0-9(),!?\\'\\`]\", \" \", string)     \n",
    "    string = re.sub(r\"\\'s\", \" \\'s\", string) \n",
    "    string = re.sub(r\"\\'ve\", \" \\'ve\", string) \n",
    "    string = re.sub(r\"n\\'t\", \" n\\'t\", string) \n",
    "    string = re.sub(r\"\\'re\", \" \\'re\", string) \n",
    "    string = re.sub(r\"\\'d\", \" \\'d\", string) \n",
    "    string = re.sub(r\"\\'ll\", \" \\'ll\", string) \n",
    "    string = re.sub(r\",\", \" , \", string) \n",
    "    string = re.sub(r\"!\", \" ! \", string) \n",
    "    string = re.sub(r\"\\(\", \" \\( \", string) \n",
    "    string = re.sub(r\"\\)\", \" \\) \", string) \n",
    "    string = re.sub(r\"\\?\", \" \\? \", string) \n",
    "    string = re.sub(r\"\\s{2,}\", \" \", string)    \n",
    "    return string.strip().lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_data_train_test(train_ratio = 0.8, clean_string=True):\n",
    "    \"\"\"\n",
    "    Loads data and split into train and test sets.\n",
    "    \"\"\"\n",
    "    revs = []\n",
    "    vocab = defaultdict(float)\n",
    "    # Pre-process train data set\n",
    "    for i in xrange(len(text_train)):\n",
    "        line = text_train[i]\n",
    "        #helpful\n",
    "        y_helpful = helpful_train[i]\n",
    "        y_helpful_cat = np.zeros(5)\n",
    "        y_helpful_cat[helpful_cat_train[i]] =1\n",
    "        #staff\n",
    "        y_staff = staff_train[i]\n",
    "        y_staff_cat = np.zeros(5)\n",
    "        y_staff_cat[staff_cat_train[i]] =1\n",
    "        #knowledge\n",
    "        y_knowledge = knowledge_train[i]\n",
    "        y_knowledge_cat = np.zeros(5)\n",
    "        y_knowledge_cat[knowledge_cat_train[i]] =1\n",
    "        \n",
    "        rev = []\n",
    "        rev.append(line.strip())\n",
    "        if clean_string:\n",
    "            orig_rev = clean_str(' '.join(rev))\n",
    "        else:\n",
    "            orig_rev = ' '.join(rev).lower()\n",
    "        words = set(orig_rev.split())\n",
    "        for word in words:\n",
    "            vocab[word] += 1\n",
    "        datum  = {'y_helpful': y_helpful, \n",
    "                  'y_helpful_cat': y_helpful_cat,\n",
    "                  'y_staff': y_staff, \n",
    "                  'y_staff_cat': y_staff_cat,\n",
    "                  'y_knowledge': y_knowledge, \n",
    "                  'y_knowledge_cat': y_knowledge_cat,\n",
    "                  'text': orig_rev,\n",
    "                  'num_words': len(orig_rev.split()),\n",
    "                  'split': int(np.random.rand() < train_ratio)}\n",
    "        revs.append(datum)\n",
    "    # Pre-process test data set\n",
    "    for i in xrange(len(text_test)):\n",
    "        line = text_test[i]\n",
    "        #helpful\n",
    "        y_helpful = helpful_test[i]\n",
    "        y_helpful_cat = np.zeros(5)\n",
    "        y_helpful_cat[helpful_cat_test[i]] =1\n",
    "        #staff\n",
    "        y_staff = staff_test[i]\n",
    "        y_staff_cat = np.zeros(5)\n",
    "        y_staff_cat[staff_cat_test[i]] =1\n",
    "        #knowledge\n",
    "        y_knowledge = knowledge_test[i]\n",
    "        y_knowledge_cat = np.zeros(5)\n",
    "        y_knowledge_cat[knowledge_cat_test[i]] =1\n",
    "        \n",
    "        rev = []\n",
    "        rev.append(line.strip())\n",
    "        if clean_string:\n",
    "            orig_rev = clean_str(' '.join(rev))\n",
    "        else:\n",
    "            orig_rev = ' '.join(rev).lower()\n",
    "        words = set(orig_rev.split())\n",
    "        for word in words:\n",
    "            vocab[word] += 1\n",
    "        datum  = {'y_helpful': y_helpful, \n",
    "                  'y_helpful_cat': y_helpful_cat,\n",
    "                  'y_staff': y_staff, \n",
    "                  'y_staff_cat': y_staff_cat,\n",
    "                  'y_knowledge': y_knowledge, \n",
    "                  'y_knowledge_cat': y_knowledge_cat,\n",
    "                  'text': orig_rev,\n",
    "                  'num_words': len(orig_rev.split()),\n",
    "                  'split': -1}\n",
    "        revs.append(datum)      \n",
    "    return revs, vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data loaded!\n",
      "number of sentences: 38200\n",
      "vocab size: 36883\n",
      "max sentence length: 247\n"
     ]
    }
   ],
   "source": [
    "revs, vocab = build_data_train_test(train_ratio=0.8, clean_string=True)\n",
    "max_l = np.max(pd.DataFrame(revs)['num_words'])\n",
    "print 'data loaded!'\n",
    "print 'number of sentences: ' + str(len(revs))\n",
    "print 'vocab size: ' + str(len(vocab))\n",
    "print 'max sentence length: ' + str(max_l)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenize. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_W(word_vecs, k=300):\n",
    "    \"\"\"\n",
    "    Get word matrix. W[i] is the vector for word indexed by i\n",
    "    \"\"\"\n",
    "    vocab_size = len(word_vecs)\n",
    "    word_idx_map = dict()\n",
    "    W = np.zeros(shape=(vocab_size+1, k), dtype=np.float32)\n",
    "    W[0] = np.zeros(k, dtype=np.float32)\n",
    "    i = 1\n",
    "    for word in word_vecs:\n",
    "        W[i] = word_vecs[word]\n",
    "        word_idx_map[word] = i\n",
    "        i += 1\n",
    "    return W, word_idx_map\n",
    "\n",
    "def load_bin_vec(fname, vocab):\n",
    "    \"\"\"\n",
    "    Loads 300x1 word vecs from Google (Mikolov) word2vec\n",
    "    \"\"\"\n",
    "    word_vecs = {}\n",
    "    with open(fname, 'rb') as f:\n",
    "        header = f.readline()\n",
    "        print header\n",
    "        vocab_size, layer1_size = map(int, header.split())\n",
    "        binary_len = np.dtype('float32').itemsize * layer1_size\n",
    "        print int(2.5*vocab_size/3)\n",
    "        for line in xrange(int(2.5*vocab_size/3)): #Change this to full on a bigger machine\n",
    "            #print line\n",
    "            word = []\n",
    "            while True:\n",
    "                ch = f.read(1)\n",
    "                if ch == ' ':\n",
    "                    word = ''.join(word)\n",
    "                    break\n",
    "                if ch != '\\n':\n",
    "                    word.append(ch)   \n",
    "            if word in vocab:\n",
    "                word_vecs[word] = np.fromstring(f.read(binary_len), dtype='float32')  \n",
    "            else:\n",
    "                f.read(binary_len)\n",
    "            if (line%100000==0):\n",
    "                print line\n",
    "    return word_vecs\n",
    "\n",
    "def add_unknown_words(word_vecs, vocab, min_df=1, k=300):\n",
    "    \"\"\"\n",
    "    For words that occur in at least min_df documents, create a separate word vector.    \n",
    "    0.25 is chosen so the unknown vectors have (approximately) same variance as pre-trained ones\n",
    "    \"\"\"\n",
    "    i=0\n",
    "    for word in vocab:\n",
    "        if word not in word_vecs and vocab[word] >= min_df:\n",
    "            i=i+1\n",
    "            word_vecs[word] = np.random.uniform(-0.25,0.25,k)              \n",
    "    print \"Total words not in vec:\", i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3000000 300\n",
      "\n",
      "2500000\n",
      "0\n",
      "100000\n",
      "200000\n",
      "300000\n",
      "400000\n",
      "500000\n",
      "600000\n",
      "700000\n",
      "800000\n",
      "900000\n",
      "1000000\n",
      "1100000\n",
      "1200000\n",
      "1300000\n",
      "1400000\n",
      "1500000\n",
      "1600000\n",
      "1700000\n",
      "1800000\n",
      "1900000\n",
      "2000000\n",
      "2100000\n",
      "2200000\n",
      "2300000\n",
      "2400000\n"
     ]
    }
   ],
   "source": [
    "w2v_file = 'GoogleNews-vectors-negative300.bin'\n",
    "# w2v = {} #load bin vec here later\n",
    "w2v = load_bin_vec(w2v_file, vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total words not in vec: 14418\n"
     ]
    }
   ],
   "source": [
    "add_unknown_words(w2v, vocab)\n",
    "W, word_idx_map = get_W(w2v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset created!\n"
     ]
    }
   ],
   "source": [
    "cPickle.dump([revs, W, word_idx_map, vocab], open('text-w-map-vocab.pickle', 'wb'))\n",
    "print 'dataset created!'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_idx_from_sent(sent, word_idx_map, max_l=51):\n",
    "    \"\"\"\n",
    "    Transforms sentence into a list of indices. Pad with zeroes.\n",
    "    \"\"\"\n",
    "    x = []\n",
    "    words = sent.split()\n",
    "    for word in words:\n",
    "        if word in word_idx_map:\n",
    "            x.append(word_idx_map[word])\n",
    "    return x\n",
    "\n",
    "def make_idx_data(revs, word_idx_map, max_l=51):\n",
    "    \"\"\"\n",
    "    Transforms sentences into a 2-d matrix.\n",
    "    \"\"\"\n",
    "    train, val, test = [], [], []\n",
    "    \n",
    "    y_helpful_train, y_helpful_val, y_helpful_test = [], [] , []\n",
    "    y_helpful_cat_train, y_helpful_cat_val, y_helpful_cat_test = [], [] , []\n",
    "\n",
    "    y_staff_train, y_staff_val, y_staff_test = [], [] , []\n",
    "    y_staff_cat_train, y_staff_cat_val, y_staff_cat_test = [], [] , []\n",
    "\n",
    "    y_knowledge_train, y_knowledge_val, y_knowledge_test = [], [] , []\n",
    "    y_knowledge_cat_train, y_knowledge_cat_val, y_knowledge_cat_test = [], [] , []\n",
    "    \n",
    "    for rev in revs:\n",
    "        sent = get_idx_from_sent(rev['text'], word_idx_map, max_l)\n",
    "        if rev['split'] == 1:\n",
    "            train.append(sent)\n",
    "            #helpful\n",
    "            y_helpful_train.append(int(rev[\"y_helpful\"]))\n",
    "            y_helpful_cat_train.append(rev[\"y_helpful_cat\"])\n",
    "            #staff\n",
    "            y_staff_train.append(int(rev[\"y_staff\"]))\n",
    "            y_staff_cat_train.append(rev[\"y_staff_cat\"])\n",
    "            #knowledge\n",
    "            y_knowledge_train.append(int(rev[\"y_knowledge\"]))\n",
    "            y_knowledge_cat_train.append(rev[\"y_knowledge_cat\"])\n",
    "        elif rev['split'] == 0:\n",
    "            val.append(sent)\n",
    "            #helpful\n",
    "            y_helpful_val.append(int(rev[\"y_helpful\"]))\n",
    "            y_helpful_cat_val.append(rev[\"y_helpful_cat\"])\n",
    "            #staff\n",
    "            y_staff_val.append(int(rev[\"y_staff\"]))\n",
    "            y_staff_cat_val.append(rev[\"y_staff_cat\"])\n",
    "            #knowledge\n",
    "            y_knowledge_val.append(int(rev[\"y_knowledge\"]))\n",
    "            y_knowledge_cat_val.append(rev[\"y_knowledge_cat\"])\n",
    "        elif rev['split'] == -1:\n",
    "            test.append(sent)\n",
    "            #helpful\n",
    "            y_helpful_test.append(int(rev[\"y_helpful\"]))\n",
    "            y_helpful_cat_test.append(rev[\"y_helpful_cat\"])\n",
    "            #staff\n",
    "            y_staff_test.append(int(rev[\"y_staff\"]))\n",
    "            y_staff_cat_test.append(rev[\"y_staff_cat\"])\n",
    "            #knowledge\n",
    "            y_knowledge_test.append(int(rev[\"y_knowledge\"]))\n",
    "            y_knowledge_cat_test.append(rev[\"y_knowledge_cat\"])\n",
    "    \n",
    "    train = sequence.pad_sequences(train, maxlen=max_l)\n",
    "    y_helpful_train = np.array(y_helpful_train, dtype=np.int)\n",
    "    y_helpful_cat_train = np.array(y_helpful_cat_train, dtype=np.int)\n",
    "    y_staff_train = np.array(y_staff_train, dtype=np.int)\n",
    "    y_staff_cat_train = np.array(y_staff_cat_train, dtype=np.int)\n",
    "    y_knowledge_train = np.array(y_knowledge_train, dtype=np.int)\n",
    "    y_knowledge_cat_train = np.array(y_knowledge_cat_train, dtype=np.int)\n",
    "      \n",
    "    val = sequence.pad_sequences(val, maxlen=max_l)\n",
    "    y_helpful_val = np.array(y_helpful_val, dtype=np.int)\n",
    "    y_helpful_cat_val = np.array(y_helpful_cat_val, dtype=np.int)\n",
    "    y_staff_val = np.array(y_staff_val, dtype=np.int)\n",
    "    y_staff_cat_val = np.array(y_staff_cat_val, dtype=np.int)\n",
    "    y_knowledge_val = np.array(y_knowledge_val, dtype=np.int)\n",
    "    y_knowledge_cat_val = np.array(y_knowledge_cat_val, dtype=np.int)\n",
    "\n",
    "    test = sequence.pad_sequences(test, maxlen=max_l)\n",
    "    y_helpful_test = np.array(y_helpful_test, dtype=np.int)\n",
    "    y_helpful_cat_test = np.array(y_helpful_cat_test, dtype=np.int)\n",
    "    y_staff_test = np.array(y_staff_test, dtype=np.int)\n",
    "    y_staff_cat_test = np.array(y_staff_cat_test, dtype=np.int)\n",
    "    y_knowledge_test = np.array(y_knowledge_test, dtype=np.int)\n",
    "    y_knowledge_cat_test = np.array(y_knowledge_cat_test, dtype=np.int)\n",
    "    \n",
    "    return [train, val, test, y_helpful_train, y_helpful_val, y_helpful_test, \n",
    "                              y_helpful_cat_train, y_helpful_cat_val, y_helpful_cat_test,\n",
    "                              y_staff_train, y_staff_val, y_staff_test, \n",
    "                              y_staff_cat_train, y_staff_cat_val, y_staff_cat_test,\n",
    "                              y_knowledge_train, y_knowledge_val, y_knowledge_test, \n",
    "                              y_knowledge_cat_train, y_knowledge_cat_val, y_knowledge_cat_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "datasets = make_idx_data(revs, word_idx_map, max_l=247)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset created!\n"
     ]
    }
   ],
   "source": [
    "cPickle.dump(datasets, open('train-test-val.pickle', 'wb'))\n",
    "print 'dataset created!'"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
