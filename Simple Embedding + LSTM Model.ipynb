{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/abaghel/projects/rnn-patient-sentiment-paper\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "print os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import cPickle\n",
    "import numpy as np\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Dropout, Activation, Flatten, Reshape, Merge\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.layers.convolutional import Convolution2D, MaxPooling2D\n",
    "from keras.optimizers import SGD, Adadelta\n",
    "from keras.constraints import unitnorm, maxnorm\n",
    "from keras.regularizers import l2\n",
    "from keras.layers.advanced_activations import PReLU\n",
    "from keras.preprocessing import sequence\n",
    "\n",
    "from keras.layers import Dense, Dropout, Activation, Embedding\n",
    "from keras.layers import LSTM, SimpleRNN, GRU\n",
    "\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "# This import to resolve some errors with tf version on office server\n",
    "import tensorflow as tf\n",
    "tf.python.control_flow_ops = tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Simple Preprocessing without word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import cPickle\n",
    "from collections import defaultdict\n",
    "import re\n",
    "\n",
    "\n",
    "def RepresentsInt(s):\n",
    "    try: \n",
    "        int(s)\n",
    "        return True\n",
    "    except ValueError:\n",
    "        return False\n",
    "\n",
    "     \n",
    "def build_data_train_test(train_ratio = 0.8, clean_string=True):\n",
    "    \"\"\"\n",
    "    Loads data and split into train and test sets.\n",
    "    \"\"\"\n",
    "    revs = []\n",
    "    vocab = defaultdict(float)\n",
    "    # Pre-process train data set\n",
    "    for i in xrange(len(text_train)):\n",
    "        line = text_train[i]\n",
    "        y = helpfull_train[i]\n",
    "        rev = []\n",
    "        rev.append(line.strip())\n",
    "        if clean_string:\n",
    "            orig_rev = clean_str(' '.join(rev))\n",
    "        else:\n",
    "            orig_rev = ' '.join(rev).lower()\n",
    "        words = set(orig_rev.split())\n",
    "        for word in words:\n",
    "            vocab[word] += 1\n",
    "        datum  = {'y': y, \n",
    "                  'text': orig_rev,\n",
    "                  'num_words': len(orig_rev.split()),\n",
    "                  'split': int(np.random.rand() < train_ratio)}\n",
    "        revs.append(datum)\n",
    "    # Pre-process test data set\n",
    "    for i in xrange(len(text_test)):\n",
    "        line = text_test[i]\n",
    "        y = helpfull_test[i]\n",
    "        rev = []\n",
    "        rev.append(line.strip())\n",
    "        if clean_string:\n",
    "            orig_rev = clean_str(' '.join(rev))\n",
    "        else:\n",
    "            orig_rev = ' '.join(rev).lower()\n",
    "        words = set(orig_rev.split())\n",
    "        for word in words:\n",
    "            vocab[word] += 1\n",
    "        datum  = {'y': y, \n",
    "                  'text': orig_rev,\n",
    "                  'num_words': len(orig_rev.split()),\n",
    "                  'split': -1}\n",
    "        revs.append(datum)      \n",
    "    return revs, vocab\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36683\n",
      "36683\n"
     ]
    }
   ],
   "source": [
    "text=[]\n",
    "helpfull=[]\n",
    "knowledge=[]\n",
    "staff=[]\n",
    "ii=0\n",
    "jj=0\n",
    "files = [os.path.join(\"data/\", f) for f in os.listdir('data/') if os.path.isfile(os.path.join(\"data/\", f))]\n",
    "for f in files:\n",
    "    if f.endswith(\".txt\"):\n",
    "        with open(f) as tsv:\n",
    "            for line in csv.reader(tsv, dialect=\"excel-tab\"):\n",
    "                ii=ii+1\n",
    "                if(len(line) > 6 and RepresentsInt(line[3]) and RepresentsInt(line[5]) and RepresentsInt(line[6]) ):\n",
    "                    jj=jj+1\n",
    "                    if int(line[5])>3:\n",
    "                        text.append(line[2])\n",
    "                        helpfull.append(int(1))\n",
    "                    elif int(line[5])<3:\n",
    "                        text.append(line[2])\n",
    "                        helpfull.append(int(0))\n",
    "                    #knowledge.append(int(line[6])-1)\n",
    "                    #staff.append(int(line[3])-1)\n",
    "    \n",
    "        \n",
    "        \n",
    "print len(helpfull)\n",
    "print len(text)\n",
    "text_train=text[:34000]\n",
    "text_test=text[34000:]\n",
    "helpfull_train=helpfull[:34000]\n",
    "helpfull_test=helpfull[34000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_W(word_vecs, k=300):\n",
    "    \"\"\"\n",
    "    Get word matrix. W[i] is the vector for word indexed by i\n",
    "    \"\"\"\n",
    "    vocab_size = len(word_vecs)\n",
    "    word_idx_map = dict()\n",
    "    W = np.zeros(shape=(vocab_size+1, k), dtype=np.float32)\n",
    "    W[0] = np.zeros(k, dtype=np.float32)\n",
    "    i = 1\n",
    "    for word in word_vecs:\n",
    "        W[i] = word_vecs[word]\n",
    "        word_idx_map[word] = i\n",
    "        i += 1\n",
    "    return W, word_idx_map\n",
    "\n",
    "def load_bin_vec(fname, vocab):\n",
    "    \"\"\"\n",
    "    Loads 300x1 word vecs from Google (Mikolov) word2vec\n",
    "    \"\"\"\n",
    "    word_vecs = {}\n",
    "    with open(fname, 'rb') as f:\n",
    "        header = f.readline()\n",
    "        print header\n",
    "        vocab_size, layer1_size = map(int, header.split())\n",
    "        binary_len = np.dtype('float32').itemsize * layer1_size\n",
    "        print vocab_size*2/3\n",
    "        for line in xrange(int(vocab_size*2.5/3)):\n",
    "            #print line\n",
    "            word = []\n",
    "            while True:\n",
    "                ch = f.read(1)\n",
    "                if ch == ' ':\n",
    "                    word = ''.join(word)\n",
    "                    break\n",
    "                if ch != '\\n':\n",
    "                    word.append(ch)   \n",
    "            if word in vocab:\n",
    "                word_vecs[word] = np.fromstring(f.read(binary_len), dtype='float32')  \n",
    "            else:\n",
    "                f.read(binary_len)\n",
    "            if (line%100000==0):\n",
    "                print line\n",
    "    return word_vecs\n",
    "\n",
    "def add_unknown_words(word_vecs, vocab, min_df=1, k=300):\n",
    "    \"\"\"\n",
    "    For words that occur in at least min_df documents, create a separate word vector.    \n",
    "    0.25 is chosen so the unknown vectors have (approximately) same variance as pre-trained ones\n",
    "    \"\"\"\n",
    "    for word in vocab:\n",
    "        if word not in word_vecs and vocab[word] >= min_df:\n",
    "            word_vecs[word] = np.random.uniform(-0.25,0.25,k)  \n",
    "\n",
    "def clean_str(string):\n",
    "    \"\"\"\n",
    "    Tokenization/string cleaning for dataset\n",
    "    Every dataset is lower cased except\n",
    "    \"\"\"\n",
    "    string = re.sub(r\"[^A-Za-z0-9(),!?\\'\\`]\", \" \", string)     \n",
    "    string = re.sub(r\"\\'s\", \" \\'s\", string) \n",
    "    string = re.sub(r\"\\'ve\", \" \\'ve\", string) \n",
    "    string = re.sub(r\"n\\'t\", \" n\\'t\", string) \n",
    "    string = re.sub(r\"\\'re\", \" \\'re\", string) \n",
    "    string = re.sub(r\"\\'d\", \" \\'d\", string) \n",
    "    string = re.sub(r\"\\'ll\", \" \\'ll\", string) \n",
    "    string = re.sub(r\",\", \" , \", string) \n",
    "    string = re.sub(r\"!\", \" ! \", string) \n",
    "    string = re.sub(r\"\\(\", \" \\( \", string) \n",
    "    string = re.sub(r\"\\)\", \" \\) \", string) \n",
    "    string = re.sub(r\"\\?\", \" \\? \", string) \n",
    "    string = re.sub(r\"\\s{2,}\", \" \", string)    \n",
    "    return string.strip().lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data loaded!\n",
      "number of sentences: 36683\n",
      "vocab size: 35971\n",
      "max sentence length: 247\n"
     ]
    }
   ],
   "source": [
    "revs, vocab = build_data_train_test(train_ratio=0.8, clean_string=True)\n",
    "max_l = np.max(pd.DataFrame(revs)['num_words'])\n",
    "print 'data loaded!'\n",
    "print 'number of sentences: ' + str(len(revs))\n",
    "print 'vocab size: ' + str(len(vocab))\n",
    "print 'max sentence length: ' + str(max_l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "w2v = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "add_unknown_words(w2v, vocab)\n",
    "W, word_idx_map = get_W(w2v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "35972\n",
      "35971\n",
      "blgh\n",
      "pasnecker\n",
      "colonoscopy\n",
      "sonja\n",
      "gag\n",
      "woods\n",
      "hanging\n",
      "woody\n",
      "localized\n",
      "dianostic\n"
     ]
    }
   ],
   "source": [
    "print (len(W))\n",
    "print (len(word_idx_map.items()))\n",
    "for i in range(10):\n",
    "    print word_idx_map.keys()[i]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_idx_from_sent(sent, word_idx_map, max_l=51, kernel_size=5):\n",
    "    \"\"\"\n",
    "    Transforms sentence into a list of indices. Pad with zeroes.\n",
    "    \"\"\"\n",
    "    x = []\n",
    "    words = sent.split()\n",
    "    for word in words:\n",
    "        if word in word_idx_map:\n",
    "            x.append(word_idx_map[word])\n",
    "#     while len(x) < max_l:\n",
    "#         x.append(0)\n",
    "    return x\n",
    "\n",
    "def make_idx_data(revs, word_idx_map, max_l=51, kernel_size=5):\n",
    "    \"\"\"\n",
    "    Transforms sentences into a 2-d matrix.\n",
    "    \"\"\"\n",
    "    train, val, test = [], [], []\n",
    "    y_train, y_val, y_test = [], [] , []\n",
    "    for rev in revs:\n",
    "        sent = get_idx_from_sent(rev['text'], word_idx_map, max_l, kernel_size)\n",
    "#         sent.append(rev['y'])\n",
    "        if rev['split'] == 1:\n",
    "            train.append(sent)\n",
    "            y_train.append(int(rev[\"y\"]))\n",
    "        elif rev['split'] == 0:\n",
    "            val.append(sent)\n",
    "            y_val.append(int(rev[\"y\"]))\n",
    "        else:\n",
    "            test.append(sent)\n",
    "            y_test.append(int(rev[\"y\"]))\n",
    "            \n",
    "            \n",
    "#     train = np.array(train, dtype=np.int)\n",
    "#     val = np.array(val, dtype=np.int)\n",
    "#     test = np.array(test, dtype=np.int)\n",
    "    train = sequence.pad_sequences(train, maxlen=max_l)\n",
    "    y_train = np.array(y_train, dtype=np.int)\n",
    "    \n",
    "    val = sequence.pad_sequences(val, maxlen=max_l)\n",
    "    y_val = np.array(y_val, dtype=np.int)\n",
    "    \n",
    "    test = sequence.pad_sequences(test, maxlen=max_l)\n",
    "    y_test = np.array(y_test, dtype=np.int)\n",
    "    \n",
    "    return [train, val, test, y_train, y_val, y_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "datasets = make_idx_data(revs, word_idx_map, max_l=200,kernel_size=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train, y_train = datasets[0], datasets[3]\n",
    "val, y_val = datasets[1], datasets[4]\n",
    "test, y_test = datasets[2], datasets[5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "27179 (27179, 200) 27179 (27179,) 0.686743441628\n",
      "[0 1]\n"
     ]
    }
   ],
   "source": [
    "print len(train), train.shape, len(y_train), y_train.shape, y_train.mean()\n",
    "print np.unique(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6821 (6821, 200) 6821 (6821,) 0.69154082979\n",
      "[0 1]\n"
     ]
    }
   ],
   "source": [
    "print len(val), val.shape, len(y_val), y_val.shape, y_val.mean()\n",
    "print np.unique(y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2683 (2683, 200) 2683 (2683,) 0.699217294074\n",
      "[0 1]\n"
     ]
    }
   ],
   "source": [
    "print len(test), test.shape, len(y_test), y_test.shape, y_test.mean()\n",
    "print np.unique(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Try the model same as IMDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Build model...\n"
     ]
    }
   ],
   "source": [
    "print('Build model...')\n",
    "model = Sequential()\n",
    "model.add(Embedding(len(vocab)+1, 128, dropout=0.2, mask_zero=True))\n",
    "model.add(LSTM(128, dropout_W=0.2, dropout_U=0.2))  # try using a GRU instead, for fun\n",
    "model.add(Dense(1))\n",
    "model.add(Activation('sigmoid'))\n",
    "\n",
    "# try using different optimizers and different optimizer configs\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train...\n",
      "Train on 27234 samples, validate on 6766 samples\n",
      "Epoch 1/15\n",
      "27234/27234 [==============================] - 435s - loss: 0.3745 - acc: 0.8288 - val_loss: 0.1986 - val_acc: 0.9138\n",
      "Epoch 2/15\n",
      "27234/27234 [==============================] - 432s - loss: 0.2382 - acc: 0.8960 - val_loss: 0.1803 - val_acc: 0.9228\n",
      "Epoch 3/15\n",
      "27234/27234 [==============================] - 430s - loss: 0.1947 - acc: 0.9166 - val_loss: 0.1784 - val_acc: 0.9261\n",
      "Epoch 4/15\n",
      "27234/27234 [==============================] - 431s - loss: 0.1652 - acc: 0.9286 - val_loss: 0.1705 - val_acc: 0.9291\n",
      "Epoch 5/15\n",
      "27234/27234 [==============================] - 432s - loss: 0.1459 - acc: 0.9347 - val_loss: 0.1644 - val_acc: 0.9307\n",
      "Epoch 6/15\n",
      "27234/27234 [==============================] - 427s - loss: 0.1250 - acc: 0.9439 - val_loss: 0.1808 - val_acc: 0.9289\n",
      "Epoch 7/15\n",
      "27234/27234 [==============================] - 434s - loss: 0.1227 - acc: 0.9442 - val_loss: 0.2058 - val_acc: 0.9268\n",
      "Epoch 8/15\n",
      "27234/27234 [==============================] - 427s - loss: 0.1067 - acc: 0.9504 - val_loss: 0.1718 - val_acc: 0.9308\n",
      "Epoch 9/15\n",
      "27234/27234 [==============================] - 427s - loss: 0.0971 - acc: 0.9556 - val_loss: 0.1946 - val_acc: 0.9226\n",
      "Epoch 10/15\n",
      "27234/27234 [==============================] - 431s - loss: 0.0917 - acc: 0.9594 - val_loss: 0.1813 - val_acc: 0.9298\n",
      "Epoch 11/15\n",
      "27234/27234 [==============================] - 451s - loss: 0.0846 - acc: 0.9609 - val_loss: 0.1816 - val_acc: 0.9301\n",
      "Epoch 12/15\n",
      "27234/27234 [==============================] - 430s - loss: 0.0875 - acc: 0.9606 - val_loss: 0.1975 - val_acc: 0.9288\n",
      "Epoch 13/15\n",
      "27234/27234 [==============================] - 426s - loss: 0.0764 - acc: 0.9646 - val_loss: 0.1923 - val_acc: 0.9276\n",
      "Epoch 14/15\n",
      "27234/27234 [==============================] - 436s - loss: 0.0768 - acc: 0.9639 - val_loss: 0.1922 - val_acc: 0.9271\n",
      "Epoch 15/15\n",
      "27234/27234 [==============================] - 431s - loss: 0.0699 - acc: 0.9659 - val_loss: 0.1910 - val_acc: 0.9265\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x22c6b790>"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('Train...')\n",
    "model.fit(train, y_train, batch_size=50, nb_epoch=15,validation_data=(val, y_val))\n",
    "# score, acc = model.evaluate(test, y_test,\n",
    "#                             batch_size=50)\n",
    "# print('Test score:', score)\n",
    "# print('Test accuracy:', acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2683/2683 [==============================] - 12s    \n",
      "('Test score:', 0.20978614839125026)\n",
      "('Test accuracy:', 0.92396571182555953)\n"
     ]
    }
   ],
   "source": [
    "score, acc = model.evaluate(test, y_test,\n",
    "                            batch_size=50)\n",
    "print('Test score:', score)\n",
    "print('Test accuracy:', acc)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
