{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/abaghel/projects/rnn-patient-sentiment-paper\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "print os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import cPickle\n",
    "import numpy as np\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Dropout, Activation, Flatten, Reshape, Merge\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.layers.convolutional import Convolution2D, MaxPooling2D\n",
    "from keras.optimizers import SGD, Adadelta\n",
    "from keras.constraints import unitnorm, maxnorm\n",
    "from keras.regularizers import l2\n",
    "from keras.layers.advanced_activations import PReLU\n",
    "from keras.preprocessing import sequence\n",
    "\n",
    "from keras.layers import Dense, Dropout, Activation, Embedding\n",
    "from keras.layers import LSTM, SimpleRNN, GRU\n",
    "\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "# This import to resolve some errors with tf version on office server\n",
    "import tensorflow as tf\n",
    "tf.python.control_flow_ops = tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Simple Preprocessing without word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.  0.  0.  1.  0.]\n"
     ]
    }
   ],
   "source": [
    "y_cat = np.zeros(5)\n",
    "y_cat[3] = 1\n",
    "print y_cat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import cPickle\n",
    "from collections import defaultdict\n",
    "import re\n",
    "\n",
    "\n",
    "def RepresentsInt(s):\n",
    "    try: \n",
    "        int(s)\n",
    "        return True\n",
    "    except ValueError:\n",
    "        return False\n",
    "\n",
    "     \n",
    "def build_data_train_test(train_ratio = 0.8, clean_string=True):\n",
    "    \"\"\"\n",
    "    Loads data and split into train and test sets.\n",
    "    \"\"\"\n",
    "    revs = []\n",
    "    vocab = defaultdict(float)\n",
    "    # Pre-process train data set\n",
    "    for i in xrange(len(text_train)):\n",
    "        line = text_train[i]\n",
    "        y = helpfull_train[i]\n",
    "        y_cat = np.zeros(5)\n",
    "        y_cat[helpful_cat_train[i]] =1\n",
    "        rev = []\n",
    "        rev.append(line.strip())\n",
    "        if clean_string:\n",
    "            orig_rev = clean_str(' '.join(rev))\n",
    "        else:\n",
    "            orig_rev = ' '.join(rev).lower()\n",
    "        words = set(orig_rev.split())\n",
    "        for word in words:\n",
    "            vocab[word] += 1\n",
    "        datum  = {'y': y, \n",
    "                  'y_cat': y_cat,\n",
    "                  'text': orig_rev,\n",
    "                  'num_words': len(orig_rev.split()),\n",
    "                  'split': int(np.random.rand() < train_ratio)}\n",
    "        revs.append(datum)\n",
    "    # Pre-process test data set\n",
    "    for i in xrange(len(text_test)):\n",
    "        line = text_test[i]\n",
    "        y = helpfull_test[i]    \n",
    "        y_cat = np.zeros(5)\n",
    "        y_cat[helpful_cat_test[i]] =1\n",
    "        rev = []\n",
    "        rev.append(line.strip())\n",
    "        if clean_string:\n",
    "            orig_rev = clean_str(' '.join(rev))\n",
    "        else:\n",
    "            orig_rev = ' '.join(rev).lower()\n",
    "        words = set(orig_rev.split())\n",
    "        for word in words:\n",
    "            vocab[word] += 1\n",
    "        datum  = {'y': y, \n",
    "                  'y_cat': y_cat,\n",
    "                  'text': orig_rev,\n",
    "                  'num_words': len(orig_rev.split()),\n",
    "                  'split': -1}\n",
    "        revs.append(datum)      \n",
    "    return revs, vocab\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "38200\n",
      "38200\n",
      "38200\n"
     ]
    }
   ],
   "source": [
    "text=[]\n",
    "helpfull=[]\n",
    "helpful_categorical = []\n",
    "knowledge=[]\n",
    "staff=[]\n",
    "ii=0\n",
    "jj=0\n",
    "files = [os.path.join(\"data/\", f) for f in os.listdir('data/') if os.path.isfile(os.path.join(\"data/\", f))]\n",
    "for f in files:\n",
    "    if f.endswith(\".txt\"):\n",
    "        with open(f) as tsv:\n",
    "            for line in csv.reader(tsv, dialect=\"excel-tab\"):\n",
    "                ii=ii+1\n",
    "                if(len(line) > 6 and RepresentsInt(line[3]) and RepresentsInt(line[5]) and RepresentsInt(line[6]) ):\n",
    "                    jj=jj+1\n",
    "                    if int(line[5])>=3:\n",
    "                        helpfull.append(int(1))\n",
    "                    elif int(line[5])<3:\n",
    "                        helpfull.append(int(0))\n",
    "                    text.append(line[2])\n",
    "                    helpful_categorical.append(int(line[5])-1)\n",
    "                    #knowledge.append(int(line[6])-1)\n",
    "                    #staff.append(int(line[3])-1)\n",
    "    \n",
    "        \n",
    "        \n",
    "print len(helpfull)\n",
    "print len(helpful_categorical)\n",
    "print len(text)\n",
    "text_train=text[:34000]\n",
    "text_test=text[34000:]\n",
    "\n",
    "helpfull_train=helpfull[:34000]\n",
    "helpfull_test=helpfull[34000:]\n",
    "\n",
    "helpful_cat_train=helpful_categorical[:34000]\n",
    "helpful_cat_test=helpful_categorical[34000:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4, 4, 4, 4, 4, 4, 4, 0, 4, 4, 4, 4, 4, 0, 4]\n"
     ]
    }
   ],
   "source": [
    "print helpful_categorical[0:15]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_W(word_vecs, k=300):\n",
    "    \"\"\"\n",
    "    Get word matrix. W[i] is the vector for word indexed by i\n",
    "    \"\"\"\n",
    "    vocab_size = len(word_vecs)\n",
    "    word_idx_map = dict()\n",
    "    W = np.zeros(shape=(vocab_size+1, k), dtype=np.float32)\n",
    "    W[0] = np.zeros(k, dtype=np.float32)\n",
    "    i = 1\n",
    "    for word in word_vecs:\n",
    "        W[i] = word_vecs[word]\n",
    "        word_idx_map[word] = i\n",
    "        i += 1\n",
    "    return W, word_idx_map\n",
    "\n",
    "def load_bin_vec(fname, vocab):\n",
    "    \"\"\"\n",
    "    Loads 300x1 word vecs from Google (Mikolov) word2vec\n",
    "    \"\"\"\n",
    "    word_vecs = {}\n",
    "    with open(fname, 'rb') as f:\n",
    "        header = f.readline()\n",
    "        print header\n",
    "        vocab_size, layer1_size = map(int, header.split())\n",
    "        binary_len = np.dtype('float32').itemsize * layer1_size\n",
    "        print vocab_size*2/3\n",
    "        for line in xrange(int(vocab_size*2.5/3)):\n",
    "            #print line\n",
    "            word = []\n",
    "            while True:\n",
    "                ch = f.read(1)\n",
    "                if ch == ' ':\n",
    "                    word = ''.join(word)\n",
    "                    break\n",
    "                if ch != '\\n':\n",
    "                    word.append(ch)   \n",
    "            if word in vocab:\n",
    "                word_vecs[word] = np.fromstring(f.read(binary_len), dtype='float32')  \n",
    "            else:\n",
    "                f.read(binary_len)\n",
    "            if (line%100000==0):\n",
    "                print line\n",
    "    return word_vecs\n",
    "\n",
    "def add_unknown_words(word_vecs, vocab, min_df=1, k=300):\n",
    "    \"\"\"\n",
    "    For words that occur in at least min_df documents, create a separate word vector.    \n",
    "    0.25 is chosen so the unknown vectors have (approximately) same variance as pre-trained ones\n",
    "    \"\"\"\n",
    "    for word in vocab:\n",
    "        if word not in word_vecs and vocab[word] >= min_df:\n",
    "            word_vecs[word] = np.random.uniform(-0.25,0.25,k)  \n",
    "\n",
    "def clean_str(string):\n",
    "    \"\"\"\n",
    "    Tokenization/string cleaning for dataset\n",
    "    Every dataset is lower cased except\n",
    "    \"\"\"\n",
    "    string = re.sub(r\"[^A-Za-z0-9(),!?\\'\\`]\", \" \", string)     \n",
    "    string = re.sub(r\"\\'s\", \" \\'s\", string) \n",
    "    string = re.sub(r\"\\'ve\", \" \\'ve\", string) \n",
    "    string = re.sub(r\"n\\'t\", \" n\\'t\", string) \n",
    "    string = re.sub(r\"\\'re\", \" \\'re\", string) \n",
    "    string = re.sub(r\"\\'d\", \" \\'d\", string) \n",
    "    string = re.sub(r\"\\'ll\", \" \\'ll\", string) \n",
    "    string = re.sub(r\",\", \" , \", string) \n",
    "    string = re.sub(r\"!\", \" ! \", string) \n",
    "    string = re.sub(r\"\\(\", \" \\( \", string) \n",
    "    string = re.sub(r\"\\)\", \" \\) \", string) \n",
    "    string = re.sub(r\"\\?\", \" \\? \", string) \n",
    "    string = re.sub(r\"\\s{2,}\", \" \", string)    \n",
    "    return string.strip().lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data loaded!\n",
      "number of sentences: 38200\n",
      "vocab size: 36883\n",
      "max sentence length: 247\n"
     ]
    }
   ],
   "source": [
    "revs, vocab = build_data_train_test(train_ratio=0.8, clean_string=True)\n",
    "max_l = np.max(pd.DataFrame(revs)['num_words'])\n",
    "print 'data loaded!'\n",
    "print 'number of sentences: ' + str(len(revs))\n",
    "print 'vocab size: ' + str(len(vocab))\n",
    "print 'max sentence length: ' + str(max_l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.,  0.,  0.,  0.,  1.])"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "revs[0][\"y_cat\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "w2v = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "add_unknown_words(w2v, vocab)\n",
    "W, word_idx_map = get_W(w2v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36884\n",
      "36883\n",
      "36883\n",
      "\n",
      "\n",
      "Some words in the vocab:\n",
      "blgh\n",
      "pasnecker\n",
      "colonoscopy\n",
      "sonja\n",
      "gag\n",
      "woods\n",
      "dogears\n",
      "hanging\n",
      "woody\n",
      "localized\n"
     ]
    }
   ],
   "source": [
    "print (len(W))\n",
    "print (len(vocab))\n",
    "print (len(word_idx_map.items()))\n",
    "print \"\\n\\nSome words in the vocab:\"\n",
    "for i in range(10):\n",
    "    print word_idx_map.keys()[i]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_idx_from_sent(sent, word_idx_map, max_l=51, kernel_size=5):\n",
    "    \"\"\"\n",
    "    Transforms sentence into a list of indices. Pad with zeroes.\n",
    "    \"\"\"\n",
    "    x = []\n",
    "    words = sent.split()\n",
    "    for word in words:\n",
    "        if word in word_idx_map:\n",
    "            x.append(word_idx_map[word])\n",
    "#     while len(x) < max_l:\n",
    "#         x.append(0)\n",
    "    return x\n",
    "\n",
    "def make_idx_data(revs, word_idx_map, max_l=51, kernel_size=5):\n",
    "    \"\"\"\n",
    "    Transforms sentences into a 2-d matrix.\n",
    "    \"\"\"\n",
    "    train, val, test = [], [], []\n",
    "    y_train, y_val, y_test = [], [] , []\n",
    "    y_cat_train, y_cat_val, y_cat_test = [], [] , []\n",
    "    for rev in revs:\n",
    "        sent = get_idx_from_sent(rev['text'], word_idx_map, max_l, kernel_size)\n",
    "#         sent.append(rev['y'])\n",
    "        if rev['split'] == 1:\n",
    "            train.append(sent)\n",
    "            y_train.append(int(rev[\"y\"]))\n",
    "            y_cat_train.append(rev[\"y_cat\"])\n",
    "        elif rev['split'] == 0:\n",
    "            val.append(sent)\n",
    "            y_val.append(int(rev[\"y\"]))\n",
    "            y_cat_val.append(rev[\"y_cat\"])\n",
    "        else:\n",
    "            test.append(sent)\n",
    "            y_test.append(int(rev[\"y\"]))\n",
    "            y_cat_test.append(rev[\"y_cat\"])\n",
    "            \n",
    "#     train = np.array(train, dtype=np.int)\n",
    "#     val = np.array(val, dtype=np.int)\n",
    "#     test = np.array(test, dtype=np.int)\n",
    "    train = sequence.pad_sequences(train, maxlen=max_l)\n",
    "    y_train = np.array(y_train, dtype=np.int)\n",
    "    y_cat_train = np.array(y_cat_train, dtype=np.int)\n",
    "    \n",
    "    val = sequence.pad_sequences(val, maxlen=max_l)\n",
    "    y_val = np.array(y_val, dtype=np.int)\n",
    "    y_cat_val = np.array(y_cat_val, dtype=np.int)\n",
    "    \n",
    "    test = sequence.pad_sequences(test, maxlen=max_l)\n",
    "    y_test = np.array(y_test, dtype=np.int)\n",
    "    y_cat_test = np.array(y_cat_test, dtype=np.int)\n",
    "    \n",
    "    return [train, val, test, y_train, y_val, y_test, y_cat_train, y_cat_val, y_cat_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "datasets = make_idx_data(revs, word_idx_map, max_l=200,kernel_size=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train, y_train, y_cat_train = datasets[0], datasets[3], datasets[6]\n",
    "val, y_val, y_cat_val = datasets[1], datasets[4], datasets[7]\n",
    "test, y_test, y_cat_test = datasets[2], datasets[5], datasets[8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "27185 (27185, 200) 27185 (27185,) 0.700018392496\n",
      "[0 1]\n",
      "[0 1]\n",
      "[ 23.58653669   6.41162406   4.09784808   5.15357734  60.75041383]\n"
     ]
    }
   ],
   "source": [
    "print len(train), train.shape, len(y_train), y_train.shape, y_train.mean()\n",
    "print np.unique(y_train)\n",
    "\n",
    "print np.unique(y_cat_train)\n",
    "print np.sum(y_cat_train, axis=0)*100/float(np.sum(y_cat_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6815 (6815, 200) 6815 (6815,) 0.70476889215\n",
      "[0 1]\n",
      "[ 23.22817315   6.29493764   3.75641966   5.06236244  61.65810712]\n"
     ]
    }
   ],
   "source": [
    "print len(val), val.shape, len(y_val), y_val.shape, y_val.mean()\n",
    "print np.unique(y_val)\n",
    "\n",
    "print np.sum(y_cat_val, axis=0)*100/float(np.sum(y_cat_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4200 (4200, 200) 4200 (4200,) 0.700476190476\n",
      "[0 1]\n",
      "[ 24.23809524   5.71428571   3.5          4.57142857  61.97619048]\n"
     ]
    }
   ],
   "source": [
    "print len(test), test.shape, len(y_test), y_test.shape, y_test.mean()\n",
    "print np.unique(y_test)\n",
    "\n",
    "print np.sum(y_cat_test, axis=0)*100/float(np.sum(y_cat_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Try the model same as IMDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Build model...\n"
     ]
    }
   ],
   "source": [
    "print('Build model...')\n",
    "model = Sequential()\n",
    "model.add(Embedding(len(vocab)+1, 128, dropout=0.2, mask_zero=True))\n",
    "model.add(LSTM(128, dropout_W=0.2, dropout_U=0.2))  # try using a GRU instead, for fun\n",
    "model.add(Dense(1))\n",
    "model.add(Activation('sigmoid'))\n",
    "\n",
    "# try using different optimizers and different optimizer configs\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train...\n",
      "Train on 27234 samples, validate on 6766 samples\n",
      "Epoch 1/15\n",
      "27234/27234 [==============================] - 435s - loss: 0.3745 - acc: 0.8288 - val_loss: 0.1986 - val_acc: 0.9138\n",
      "Epoch 2/15\n",
      "27234/27234 [==============================] - 432s - loss: 0.2382 - acc: 0.8960 - val_loss: 0.1803 - val_acc: 0.9228\n",
      "Epoch 3/15\n",
      "27234/27234 [==============================] - 430s - loss: 0.1947 - acc: 0.9166 - val_loss: 0.1784 - val_acc: 0.9261\n",
      "Epoch 4/15\n",
      "27234/27234 [==============================] - 431s - loss: 0.1652 - acc: 0.9286 - val_loss: 0.1705 - val_acc: 0.9291\n",
      "Epoch 5/15\n",
      "27234/27234 [==============================] - 432s - loss: 0.1459 - acc: 0.9347 - val_loss: 0.1644 - val_acc: 0.9307\n",
      "Epoch 6/15\n",
      "27234/27234 [==============================] - 427s - loss: 0.1250 - acc: 0.9439 - val_loss: 0.1808 - val_acc: 0.9289\n",
      "Epoch 7/15\n",
      "27234/27234 [==============================] - 434s - loss: 0.1227 - acc: 0.9442 - val_loss: 0.2058 - val_acc: 0.9268\n",
      "Epoch 8/15\n",
      "27234/27234 [==============================] - 427s - loss: 0.1067 - acc: 0.9504 - val_loss: 0.1718 - val_acc: 0.9308\n",
      "Epoch 9/15\n",
      "27234/27234 [==============================] - 427s - loss: 0.0971 - acc: 0.9556 - val_loss: 0.1946 - val_acc: 0.9226\n",
      "Epoch 10/15\n",
      "27234/27234 [==============================] - 431s - loss: 0.0917 - acc: 0.9594 - val_loss: 0.1813 - val_acc: 0.9298\n",
      "Epoch 11/15\n",
      "27234/27234 [==============================] - 451s - loss: 0.0846 - acc: 0.9609 - val_loss: 0.1816 - val_acc: 0.9301\n",
      "Epoch 12/15\n",
      "27234/27234 [==============================] - 430s - loss: 0.0875 - acc: 0.9606 - val_loss: 0.1975 - val_acc: 0.9288\n",
      "Epoch 13/15\n",
      "27234/27234 [==============================] - 426s - loss: 0.0764 - acc: 0.9646 - val_loss: 0.1923 - val_acc: 0.9276\n",
      "Epoch 14/15\n",
      "27234/27234 [==============================] - 436s - loss: 0.0768 - acc: 0.9639 - val_loss: 0.1922 - val_acc: 0.9271\n",
      "Epoch 15/15\n",
      "27234/27234 [==============================] - 431s - loss: 0.0699 - acc: 0.9659 - val_loss: 0.1910 - val_acc: 0.9265\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x22c6b790>"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('Train...')\n",
    "model.fit(train, y_train, batch_size=50, nb_epoch=15,validation_data=(val, y_val))\n",
    "# score, acc = model.evaluate(test, y_test,\n",
    "#                             batch_size=50)\n",
    "# print('Test score:', score)\n",
    "# print('Test accuracy:', acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2683/2683 [==============================] - 12s    \n",
      "('Test score:', 0.20978614839125026)\n",
      "('Test accuracy:', 0.92396571182555953)\n"
     ]
    }
   ],
   "source": [
    "score, acc = model.evaluate(test, y_test,\n",
    "                            batch_size=50)\n",
    "print('Test score:', score)\n",
    "print('Test accuracy:', acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## =================================================\n",
    "## Try categorial classification "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Build model_cat...\n"
     ]
    }
   ],
   "source": [
    "print('Build model_cat...')\n",
    "model_cat = Sequential()\n",
    "model_cat.add(Embedding(len(vocab)+1, 128, dropout=0.2, mask_zero=True))\n",
    "model_cat.add(LSTM(128, dropout_W=0.2, dropout_U=0.2))  # try using a GRU instead, for fun\n",
    "model_cat.add(Dense(5))\n",
    "model_cat.add(Activation('sigmoid'))\n",
    "\n",
    "# try using different optimizers and different optimizer configs\n",
    "model_cat.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python2.7/site-packages/tensorflow/python/ops/gradients.py:90: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 27185 samples, validate on 6815 samples\n",
      "Epoch 1/15\n",
      "27185/27185 [==============================] - 434s - loss: 0.9720 - acc: 0.6714 - val_loss: 0.7056 - val_acc: 0.7837\n",
      "Epoch 2/15\n",
      "27185/27185 [==============================] - 430s - loss: 0.6996 - acc: 0.7749 - val_loss: 0.6225 - val_acc: 0.7972\n",
      "Epoch 3/15\n",
      "27185/27185 [==============================] - 424s - loss: 0.6194 - acc: 0.7943 - val_loss: 0.6040 - val_acc: 0.8006\n",
      "Epoch 4/15\n",
      "27185/27185 [==============================] - 422s - loss: 0.5648 - acc: 0.8070 - val_loss: 0.5951 - val_acc: 0.8016\n",
      "Epoch 5/15\n",
      "27185/27185 [==============================] - 423s - loss: 0.5144 - acc: 0.8193 - val_loss: 0.6145 - val_acc: 0.7943\n",
      "Epoch 6/15\n",
      "27185/27185 [==============================] - 418s - loss: 0.4677 - acc: 0.8333 - val_loss: 0.6403 - val_acc: 0.7941\n",
      "Epoch 7/15\n",
      "27185/27185 [==============================] - 408s - loss: 0.4225 - acc: 0.8487 - val_loss: 0.6746 - val_acc: 0.7843\n",
      "Epoch 8/15\n",
      "27185/27185 [==============================] - 407s - loss: 0.3833 - acc: 0.8627 - val_loss: 0.6984 - val_acc: 0.7840\n",
      "Epoch 9/15\n",
      "27185/27185 [==============================] - 401s - loss: 0.3456 - acc: 0.8709 - val_loss: 0.7176 - val_acc: 0.7705\n",
      "Epoch 10/15\n",
      "27185/27185 [==============================] - 402s - loss: 0.3091 - acc: 0.8875 - val_loss: 0.7985 - val_acc: 0.7759\n",
      "Epoch 11/15\n",
      "27185/27185 [==============================] - 407s - loss: 0.2843 - acc: 0.8967 - val_loss: 0.8007 - val_acc: 0.7749\n",
      "Epoch 12/15\n",
      "27185/27185 [==============================] - 403s - loss: 0.2549 - acc: 0.9073 - val_loss: 0.8713 - val_acc: 0.7618\n",
      "Epoch 13/15\n",
      "27185/27185 [==============================] - 405s - loss: 0.2375 - acc: 0.9136 - val_loss: 0.8611 - val_acc: 0.7736\n",
      "Epoch 14/15\n",
      "27185/27185 [==============================] - 403s - loss: 0.2173 - acc: 0.9205 - val_loss: 0.9455 - val_acc: 0.7671\n",
      "Epoch 15/15\n",
      "27185/27185 [==============================] - 403s - loss: 0.2078 - acc: 0.9239 - val_loss: 0.9429 - val_acc: 0.7705\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x146eea90>"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('Train...')\n",
    "\n",
    "model_cat.fit(train, y_cat_train, batch_size=50, nb_epoch=15,validation_data=(val, y_cat_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Build model_cat...\n"
     ]
    }
   ],
   "source": [
    "print('Build model_cat...')\n",
    "model_cat = Sequential()\n",
    "model_cat.add(Embedding(len(vocab)+1, 128, dropout=0.2, mask_zero=True))\n",
    "model_cat.add(LSTM(128, dropout_W=0.2, dropout_U=0.2))  # try using a GRU instead, for fun\n",
    "model_cat.add(Dense(5))\n",
    "model_cat.add(Activation('sigmoid'))\n",
    "\n",
    "# try using different optimizers and different optimizer configs\n",
    "model_cat.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy', 'mean_absolute_error'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train...\n",
      "Train on 27185 samples, validate on 6815 samples\n",
      "Epoch 1/15\n",
      "27185/27185 [==============================] - 410s - loss: 0.9700 - acc: 0.6667 - mean_absolute_error: 0.2196 - val_loss: 0.6900 - val_acc: 0.7839 - val_mean_absolute_error: 0.1458\n",
      "Epoch 2/15\n",
      "27185/27185 [==============================] - 415s - loss: 0.6899 - acc: 0.7769 - mean_absolute_error: 0.1420 - val_loss: 0.6333 - val_acc: 0.7957 - val_mean_absolute_error: 0.1401\n",
      "Epoch 3/15\n",
      "27185/27185 [==============================] - 410s - loss: 0.6244 - acc: 0.7931 - mean_absolute_error: 0.1340 - val_loss: 0.6245 - val_acc: 0.7957 - val_mean_absolute_error: 0.1196\n",
      "Epoch 4/15\n",
      "27185/27185 [==============================] - 408s - loss: 0.5636 - acc: 0.8055 - mean_absolute_error: 0.1259 - val_loss: 0.5992 - val_acc: 0.8012 - val_mean_absolute_error: 0.1225\n",
      "Epoch 5/15\n",
      "27185/27185 [==============================] - 406s - loss: 0.5160 - acc: 0.8212 - mean_absolute_error: 0.1241 - val_loss: 0.6112 - val_acc: 0.7979 - val_mean_absolute_error: 0.1232\n",
      "Epoch 6/15\n",
      "27185/27185 [==============================] - 408s - loss: 0.4703 - acc: 0.8322 - mean_absolute_error: 0.1205 - val_loss: 0.6560 - val_acc: 0.7897 - val_mean_absolute_error: 0.1229\n",
      "Epoch 7/15\n",
      "27185/27185 [==============================] - 405s - loss: 0.4313 - acc: 0.8436 - mean_absolute_error: 0.1210 - val_loss: 0.6422 - val_acc: 0.7899 - val_mean_absolute_error: 0.1269\n",
      "Epoch 8/15\n",
      "27185/27185 [==============================] - 407s - loss: 0.3879 - acc: 0.8578 - mean_absolute_error: 0.1183 - val_loss: 0.6970 - val_acc: 0.7887 - val_mean_absolute_error: 0.1200\n",
      "Epoch 9/15\n",
      "27185/27185 [==============================] - 406s - loss: 0.3581 - acc: 0.8706 - mean_absolute_error: 0.1151 - val_loss: 0.6975 - val_acc: 0.7808 - val_mean_absolute_error: 0.1294\n",
      "Epoch 10/15\n",
      "27185/27185 [==============================] - 405s - loss: 0.3236 - acc: 0.8824 - mean_absolute_error: 0.1124 - val_loss: 0.7530 - val_acc: 0.7564 - val_mean_absolute_error: 0.1352\n",
      "Epoch 11/15\n",
      "  550/27185 [..............................] - ETA: 369s - loss: 0.3128 - acc: 0.8764 - mean_absolute_error: 0.1204"
     ]
    }
   ],
   "source": [
    "print('Train...')\n",
    "\n",
    "model_cat.fit(train, y_cat_train, batch_size=50, nb_epoch=15,validation_data=(val, y_cat_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
